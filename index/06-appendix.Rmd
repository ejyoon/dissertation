`r if(knitr:::is_latex_output()) '\\appendix'`

`r if(!knitr:::is_latex_output()) '# (APPENDIX) Appendix {-}'` 

<!--
If you feel it necessary to include an appendix, it goes here.
-->

```{r, include=FALSE}
knitr::opts_chunk$set(echo = F, cache = T, warning=F, message=F, 
                      fig.width=4, fig.asp = 0.618, out.width = "75%", 
                      fig.align = 'center', fig.pos = 't')
```

# Supplementary materials for Chapter 2

In this appendix, we present four pieces of supplemental information. First, we provide details about the Bayesian models used to analyze the data. Second, we present a sensitivity analysis that provides evidence that the estimates of the associations between age/vocabulary and accuracy/reaction time (RT) are robust to different parameterizations of the prior distribution and different cutoffs for the analysis window. Third, we present the results of a parallel set of analyses using a non-Bayesian approach to show that these results are consistent regardless of choice of analytic framework. And fourth, we present two exploratory analyses measuring the effects of phonological overlap and iconicity on RT and accuracy. In both analyses, we did not see evidence that these factors changed the dynamics of eye movements during ASL processing

## Model Specifications

Our key analyses use Bayesian linear models to test our hypotheses of interest and to estimate the associations between age/vocabulary and RT/accuracy. Figure S1 (Accuracy) and S2 (RT) present graphical models that represent all of the data, parameters, and other variables of interest, and their dependencies. Latent parameters are shown as unshaded nodes while observed parameters and data are shown as shaded nodes. All models were fit using JAGS software (Plummer, 2003) and adapted from models in Kruschke (2014) and Lee and Wagenmakers (2014).

### Accuracy

```{r, fig.cap = "Graphical model representation of the linear regression used to predict accuracy. The shaded nodes represent observed data (i.e., each participant's age, vocabulary, and mean accuracy). Unshaded nodes represent latent parameters (i.e., the intercept and slope of the linear model).", fig.scap="Graphical representation of the accuracy model."}

include_graphics(path = here::here(data_code_path_sol, "figures/acc_model.png"))
```

To test the association between age/vocabulary and accuracy we assume each participant's mean accuracy is drawn from a Gaussian distribution with a mean, $\mu$, and a standard deviation, $\sigma$. The mean is a linear function of the intercept, $\alpha$, which encodes the expected value of the outcome variable when the predictor is zero, and the slope, $\beta$, which encodes the expected change in the outcome with each unit change in the predictor (i.e., the strength of association). 


For $\alpha$ and $\sigma$, we use vague priors on a standardized scale, allowing the model to consider a wide range of plausible values. Since the slope parameter $\beta$ is critical to our hypothesis of a linear association, we chose to use an informed prior: that is, a truncated Gaussian distribution with a mean of zero and a standard deviation of one on a standardized scale. Centering the distribution at zero is conservative and places the highest prior probability on a null association, to reduce the chance that our model overfits the data. Truncating the prior encodes our directional hypothesis that accuracy should increase with age and larger vocabulary size. And using a standard deviation of one constrains the plausible slope values, thus making our alternative hypothesis more precise. We constrained the slope values based on previous research with children learning spoken language showing that the average gain in accuracy for one month of development between 18-24 months to be ~1.5%  (Fernald, Zangl, Portillo, & Marchman, 2008). 

### Reaction Time

```{r, fig.cap = "Graphical model representation of the linear regression plus latent mixture model (i.e., guessing model). The model assumes that each individual participant's first shift is either the result of guessing or knowledge. And the latent indicator $z_i$ determines whether that participant is included in the linear regression estimating the association between age/vocabulary and RT.", fig.scap="Graphical representation of the RT model."}

include_graphics(path = here::here(data_code_path_sol, "figures/rt_model.png"))
```

The use of RT as a processing measure is based on the assumption that the timing of a child's first shift reflects the speed of their incremental language comprehension. Yet, some children have a first shift that seems to be unassociated with this construct: their first shift behavior appears random. We quantify this possibility for each participant explicitly (i.e., the probability that the participant is a "guesser") and we create an analysis model where participants who were more likely to be guessers have less of an influence on the estimated relations between RT and age/vocabulary.

To quantify each participant's probability of guessing, we computed the proportion of signer-to-target (correct) and signer-to-distracter (incorrect) shifts for each child. We then used a latent mixture model in which we assumed that the observed data, k_i, were generated by two processes (guessing and knowledge) that had different overall probabilities of success, with the "guessing group" having a probability of 50%, ψ, and the "knowledge" group having a probability greater than 50%, $\phi$. The group membership of each participant is a latent indicator variable, $z_i$, inferred based on that participant's proportion of correct signer-to-target shifts relative to the overall proportion of correct shifts across all participants (see Lee & Wagenmakers (2014) for a detailed discussion of this modeling approach). We then used each participant's inferred group membership to determine whether they were included in the linear regression. In sum, the model allows participants to contribute to the estimated associations between age/vocabulary and RT proportional to our belief that they were guessing.

As in the Accuracy model, we use vague priors for $\alpha$ and $\sigma$ on a standardized scale. We again use an informed prior for $\beta$, making our alternative hypothesis more precise. That is, we constrained the plausible slope values based on previous research with children learning spoken language showing that the average gain in RT for one month of development between 18-24 months to be ~30 ms (Fernald, Zangl, Portillo, & Marchman, 2008).

## Sensitivity Analysis: Prior Distribution and Window Selection

```{r, fig.cap = "Coefficient plot for the slope parameter for four different parameterizations of the prior and for three different analysis windows. Each panel shows a different model. Each point represents a β coefficient measuring the strength of association between the two variables. Error bars are 95\\% HDIs around the coefficient. Color represents the three different analysis windows.", fig.scap="Results of sensitivity analysis for Experiment 1.1."}

include_graphics(path = here::here(data_code_path_sol, "figures/supp_coef_plot.png"))
```

We conducted a sensitivity analysis to show that our parameter estimates for the associations between accuracy/RT and age/vocabulary are robust to decisions about (a) the analysis window and (b) the specification of the prior distribution on the slope parameter. Specifically, we varied the parameterization of the standard deviation on the slope, allowing the model to consider a wider or narrower range of values to be plausible a priori. We also fit these different models to two additional analysis windows +/- 300 ms from the final analysis window: 600-2500 ms (the middle 90% of the RT distribution in our experiment).

```{r}
d_sensitivity <- read_delim(here::here(data_code_path_sol, "data/processed_data/sol_supp_sensitivity_table.txt"), delim = "\t")

d_sensitivity %>% 
  kable("latex", 
        booktabs = T, 
        caption = "Bayes Factors for all four linear models fit to three different analysis windows using four different parameterizations of the prior distribution for the slope parameter.",
        caption.short = "Results for sensitivity analysis for Experiment 1.1") %>%
  kable_styling(latex_options = "scale_down") %>% 
  row_spec(row = 0, bold = TRUE) %>% 
  column_spec(column = 2, width = "3cm") %>% 
  column_spec(column = 1, width = "3.5cm") 
```


Figure S3 shows the results of the sensitivity analysis, plotting the coefficient for the $\beta$ parameter in each model for the three different analysis windows for each specification of the prior. All models show similar coefficient values, suggesting that inferences about the parameters are not sensitive to the exact form of the priors. Table S1 shows the Bayes Factors for all models across three analysis windows and fit using four different vales for the slope prior. The Bayes Factor only drops below 3 when the prior distribution is quite broad (standard deviation of 3.2) and only for the longest analysis window (600-2800 ms). In sum, the strength of evidence for a linear association is robust to the choice of analysis window and prior specification.

## Parallel set of non-Bayesian analyses

First, we compare Accuracy and RT of native hearing and deaf signers using a Welch Two Sample t-test and do not find evidence that these groups are different (Accuracy: t(28) = 0.75, p = 0.45, 95% CI on the difference in means [-0.07, 0.14]; RT: t(28) = 0.75, p = 0.46, 95% CI on the difference in means [-125.47 ms, 264.99 ms]. 

Second, we test whether children and adults tend to generate saccades away from the central signer prior to the offset of the target sign. To do this, we use a One Sample t-test with a null hypothesis that the true mean is not equal to 1, and we find evidence against this null (Children: M = 0.88, t(28) = -2.92, p = 0.007, 95% CI [0.79, 0.96]; Adults: M = 0.51, t(15) = -6.87, p < 0.001, 95% CI [0.35, 0.65])

Third, we fit the four linear models using MLE to estimate the relations between the processing measures on the VLP task (Accuracy/RT) and age/vocabulary. We follow recommendations from Barr (2008) and use a logistic transform to convert the proportion accuracy scores to a scale more suitable for the linear model. Table XXX shows the results. 

```{r}
d_lm <- read_delim(here::here(data_code_path_sol, "data/processed_data/sol_supp_freq_table.txt"), delim = "\t")

d_lm %>% 
  kable("latex", 
        booktabs = T, 
        caption = "Results for the four linear models fit using Maxiumum Likelihood Estimation. All p-values are one-sided to reflect our directional hypotheses about the VLP measures improving over development.",
        caption.short = "Results for MLE models") %>%
  kable_styling(latex_options = "scale_down") %>% 
  row_spec(row = 0, bold = TRUE) %>% 
  column_spec(column = 2, width = "3cm") %>% 
  column_spec(column = 1, width = "3.5cm") 
```

## Analyses of phonological overlap and iconicity

```{r, fig.cap = "Scatterplot of the association between degree of phonological overlap and RT (top row) and accuracy (bottom row) for both adults (left column) and children (right column). The blue line represents a linear model fit.", fig.scap="association between degree of phonological overlap and RT/Accuracy."}

include_graphics(path = here::here(data_code_path_sol, "figures/phonological_overlap.png"))
```

First, we analyzed whether phonological overlap of our item-pairs might have influenced adults and children’s RTs and accuracy. Signs that are higher in phonological overlap might have been more difficult to process because they are more confusable. Here, phonological overlap is quantified as the number of features (e.g., Selected Fingers, Major Location, Movement, Sign Type) that both signs shared. Values were taken from a recently created database (ASL-LEX) of lexical and phonological properties of nearly 1,000 signs of American Sign Language (Caselli et al., 2017). Our item-pairs varied in degree of overlap from 1-4 features.  We did not see evidence that degree of phonological overlap influenced either processing measure in the VLP task.

```{r, fig.cap = "Scatterplot of the association between degree of iconicity and RT (top row) and accuracy (bottom row) for both adults (left column) and children (right column). The blue line represents a linear model fit.", fig.scap="association between degree of iconicity and RT/Accuracy."}

include_graphics(path = here::here(data_code_path_sol, "figures/iconicity.png"))
```


Next, we performed a parallel analysis, exploring whether the iconicity of our signs might have influenced adults and children’s RT and accuracy. It is possible that highly iconic signs might be easier to process because of the visual similarity to the target object. Again, we used ASL-LEX to quantify the iconicity of our signs. To generate these values, native signers were asked to explicitly rate the iconicity of each sign on a scale of 1-7, with 1 being not iconic at all and 7 being very iconic. Similar to the phonological overlap analysis, we did see evidence that degree of iconicity influenced either processing measure for either age group in the VLP task.

# Supplementary materials for Chapter 4

\captionsetup[table]{labelformat=empty}

\section*{Table A1. Length of inspection times on exposure trials in Experiment 1 as a function of gaze, interval, and number of referents}
\texttt{Log(Inspection time) $\sim$ (Gaze + Log(Interval) + Log(Referents))$^2$ + (1 | subject)}

```{r e1 inspect model table, results = 'asis'}
# some code to clean up tables for paper
e1.tab.inspect <- broom::tidy(m1_rt_expt1) %>% 
  filter(group == "fixed") %>% 
  select(term:statistic) %>% 
  rename(t.value = statistic) %>% 
  mutate(p.value = 2 * (1 - pnorm(abs(t.value)))) %>% 
  mutate_at(.vars = c("estimate", "std.error", "t.value"), 
            .funs = round, digits = 2) %>% 
  mutate(p.value = ifelse(round(p.value, 3) == 0, "$<$ .001", round(p.value, 3))) %>% 
  rowwise() %>% 
  mutate(sig = getstars(p.value))

e1.tab.inspect$term <- c("Intercept","Gaze Condition","Log(Interval)",
                         "Log(Referents)","Gaze Condition*Log(Interval)", 
                         "Gaze Condition*Log(Referent)", "Log(Interval)*Log(Referent)")

names(e1.tab.inspect)[6] <- c("")

print(xtable(e1.tab.inspect,
             align = c("l","l","r","r","r","r","l"),
             label = "tab:e1_rt"),
      include.rownames=FALSE, 
      hline.after=c(0,nrow(e1.tab.inspect)),
      sanitize.text.function=function(x){x},
      table.placement = "h",
      comment = F)
```

\newpage
\section*{Table A2. Accuracy on test trials in Experiment 1 with inspection times on exposure trials included as a predictor}
\texttt{Correct $\sim$ (Trial Type + Gaze + Log(Interval) + Log(Referents) + \\ Log(Inspection Time))$^2$ + offset(logit($^1/_{Referents}$)) + (TrialType | subject)}

```{r e1 inspect model acc table, results = 'asis'}
# some code to clean up tables for paper
e1.tab.inspect.acc <- broom::tidy(m1_2way_acc_expt1_inspect) %>% 
  filter(group == "fixed") %>% 
  select(term:p.value) %>% 
  rename(z.value = statistic) %>% 
  mutate_at(.vars = c("estimate", "std.error", "z.value"), 
            .funs = round, digits = 2) %>% 
  mutate(p.value = ifelse(round(p.value, 3) == 0, "$<$ .001", round(p.value, 3))) %>% 
  rowwise() %>% 
  mutate(sig = getstars(p.value))

e1.tab.inspect.acc$term <- c("Intercept", "Switch Trial", "Gaze Condition","Log(Interval)",
                             "Log(Referents)","Log(Inspection Time)","Switch Trial*Gaze Condition",
                             "Switch Trial*Log(Interval)", "Switch Trial*Log(Referent)",
                             "Switch Trial*Log(Inspection Time)", "Gaze Condition*Log(Interval)",
                             "Gaze Condition*Log(Referent)", "Gaze Condition*Log(Inspection Time)",
                             "Log(Interval)*Log(Referent)", "Log(Interval)*Log(Inspection Time)",
                             "Log(Referents)*Log(Inspection Time)")

names(e1.tab.inspect.acc)[6] <- c("")

print(xtable(e1.tab.inspect.acc,
             align = c("l","l","r","r","r","r","l"),
             label = "tab:e1_acc_it"),
      include.rownames=FALSE,hline.after=c(0,nrow(e1.tab.inspect.acc)),
      sanitize.text.function=function(x){x},
      table.placement = "h",
      comment = F)
```

\newpage
\section*{Table A3. Length of inspection times on exposure trials in Experiment 2 as a function of gaze and interval}
\texttt{Log(Inspection time) $\sim$ Gaze * Log(Interval) + (1 | subject)}

```{r e2 inspect model table, results = 'asis'}
# some code to clean up tables for paper
e2.tab.inspect <- broom::tidy(m1_rt_expt2) %>% 
  filter(group == "fixed") %>% 
  select(term:statistic) %>% 
  rename(t.value = statistic) %>% 
  mutate(p.value = 2 * (1 - pnorm(abs(t.value)))) %>% 
  mutate_at(.vars = c("estimate", "std.error", "t.value"), 
            .funs = round, digits = 2) %>% 
  mutate(p.value = ifelse(round(p.value, 3) == 0, "$<$ .001", round(p.value, 3))) %>% 
  rowwise() %>% 
  mutate(sig = getstars(p.value))

e2.tab.inspect$term <- c("Intercept", "Gaze Condition", "Log(Interval)",
                         "Gaze Condition*Log(Interval)")

names(e2.tab.inspect)[6] <- c("")

print(xtable(e2.tab.inspect,
             align = c("l","l","r","r","r","r","l"),
             label = "tab:e2_rt"),
      include.rownames=FALSE,hline.after=c(0,nrow(e2.tab.inspect)),
      sanitize.text.function=function(x){x},
      table.placement = "h",
      comment = F)
```

\section*{Table A4. Accuracy on test trials in Experiment 2 with inspection times on exposure trials included as a predictor}
\texttt{Correct $\sim$ (Trial Type + Gaze + Log(Interval) + Log(Inspection Time))$^2$ + \\ offset(logit($^1/_{Referents}$)) + (TrialType | subject)}

```{r e2 inspect model acc table, results = 'asis'}
# some code to clean up tables for paper
e2.tab.inspect.acc <- broom::tidy(m2_inspect_e2) %>% 
  filter(group == "fixed") %>% 
  select(term:p.value) %>% 
  rename(z.value = statistic) %>% 
  mutate_at(.vars = c("estimate", "std.error", "z.value"), 
            .funs = round, digits = 2) %>% 
  mutate(p.value = ifelse(round(p.value, 3) == 0, "$<$ .001", round(p.value, 3))) %>% 
  rowwise() %>% 
  mutate(sig = getstars(p.value))

e2.tab.inspect.acc$term <- c("Intercept", "Gaze Condition", "Switch Trial", "Log(Interval)",
                             "Log(Inspection Time)", "Switch Trial*Gaze Condition",
                             "Gaze Condition*Log(Interval)", "Gaze Condition*Log(Inspection Time)",
                             "Switch Trial*Log(Interval)", "Switch Trial*Log(Inspection Time)", 
                             "Log(Interval)*Log(Inspection Time)")

names(e2.tab.inspect.acc)[6] <- c("")

print(xtable(e2.tab.inspect.acc,
             align = c("l","l","r","r","r","r","l"),
             label = "tab:e2_acc_it"),
      include.rownames=FALSE,hline.after=c(0,nrow(e2.tab.inspect.acc)),
      sanitize.text.function=function(x){x},
      table.placement = "h",
      comment = F)
```

\newpage
\section*{Table A5. Accuracy on exposure trials in Experiment 3 as a function of reliability condition and participants' subjective reliability judgments}
\texttt{Correct-Exposure $\sim$ Reliability Condition * Subjective Reliability + \\  offset(logit($^1/_{Referents}$)) + (1 | subject)}

```{r e3 gf on exposure table, results = 'asis'}
# some code to clean up tables for paper
e3.tab.gf.exposure <- broom::tidy(m1_expo_expt3) %>% 
filter(group == "fixed") %>% 
select(term:p.value) %>% 
rename(z.value = statistic) %>% 
mutate_at(.vars = c("estimate", "std.error", "z.value"), 
.funs = round, digits = 2) %>% 
mutate(p.value = ifelse(round(p.value, 3) == 0, "$<$ .001", round(p.value, 3))) %>% 
rowwise() %>% 
mutate(sig = getstars(p.value))

e3.tab.gf.exposure$term <- c("Intercept", "Reliability Condition", "Subjective Reliability",
"Reliability Condition*Subjective Reliability")

names(e3.tab.gf.exposure)[6] <- c("")

print(xtable(e3.tab.gf.exposure,
align = c("l","l","r","r","r","r","l"),
label = "tab:e3_gf_exp"),
include.rownames=FALSE,hline.after=c(0,nrow(e3.tab.gf.exposure)),
sanitize.text.function=function(x){x},
table.placement = "h",
comment = F)
```

\section*{Table A6. Accuracy on test trials in Experiment 3 as a function of reliability condition}
\texttt{Correct $\sim$ Trial Type * Reliability Condition + offset(logit($^1/_{Referents}$)) + \\ (Trial Type | subject)}

```{r e3 acc test rel cond table, results = 'asis'}
# some code to clean up tables for paper
e3.tab.acc.rel.cond <- broom::tidy(m1_expt3) %>% 
filter(group == "fixed") %>% 
select(term:p.value) %>% 
rename(z.value = statistic) %>% 
mutate_at(.vars = c("estimate", "std.error", "z.value"), 
.funs = round, digits = 2) %>% 
mutate(p.value = ifelse(round(p.value, 3) == 0, "$<$ .001", round(p.value, 3))) %>% 
rowwise() %>% 
mutate(sig = getstars(p.value))

e3.tab.acc.rel.cond$term <- c("Intercept", "Trial Type", "Reliability Condition",
"Reliability Condition*Trial Type")

names(e3.tab.acc.rel.cond)[6] <- c("")

print(xtable(e3.tab.acc.rel.cond,
align = c("l","l","r","r","r","r","l"),
label = "tab:e3_acc_rel_cond"),
include.rownames=FALSE,hline.after=c(0,nrow(e3.tab.acc.rel.cond)),
sanitize.text.function=function(x){x},
table.placement = "h",
comment = F)
```

\newpage
\section*{Table A7. Accuracy on test trials in Experiment 3 as a function of reliability condition and participants' use of gaze on exposure trials}
\texttt{Correct $\sim$ (Trial Type + Reliability Condition + Correct-Exposure)$^2$ \\ + offset(logit($^1/_{Referents}$)) + (Trial Type | subject)}

```{r e3 acc test rel and gaze follow table, results = 'asis'}
# some code to clean up tables for paper
e3.tab.acc.rel.cond.gf <- broom::tidy(m2a_expt3) %>% 
  filter(group == "fixed") %>% 
  select(term:p.value) %>% 
  rename(z.value = statistic) %>% 
  mutate_at(.vars = c("estimate", "std.error", "z.value"), 
            .funs = round, digits = 2) %>% 
  mutate(p.value = ifelse(round(p.value, 3) == 0, "$<$ .001", round(p.value, 3))) %>% 
  rowwise() %>% 
  mutate(sig = getstars(p.value))

e3.tab.acc.rel.cond.gf$term <- c("Intercept", "Correct Exposure", "Trial Type", "Reliability Condition",
                                 "Correct Exposure*Trial Type", "Correct Exposure*Reliability",
                                 "Reliability Condition*Trial Type")

names(e3.tab.acc.rel.cond.gf)[6] <- c("")

print(xtable(e3.tab.acc.rel.cond.gf,
             align = c("l","l","r","r","r","r","l"),
             label = "tab:e3_acc_rel_cond_gf"),
      include.rownames=FALSE,hline.after=c(0,nrow(e3.tab.acc.rel.cond.gf)),
      sanitize.text.function=function(x){x},
      table.placement = "h",
      comment = F)
```

\section*{Table A8. Accuracy on test trials in Experiment 3 as a function of each participants' accuracy on exposure trials}
\texttt{Correct $\sim$ Trial Type * Total Correct Exposure + offset(logit($^1/_{Referents}$)) + \\ (Trial Type | subject)}

```{r e3 acc test gaze use table, results = 'asis'}
# some code to clean up tables for paper
e3.tab.acc.gaze.use <- broom::tidy(m2b_expt3) %>% 
  filter(group == "fixed") %>% 
  select(term:p.value) %>% 
  rename(z.value = statistic) %>% 
  mutate_at(.vars = c("estimate", "std.error", "z.value"), 
            .funs = round, digits = 2) %>% 
  mutate(p.value = ifelse(round(p.value, 3) == 0, "$<$ .001", round(p.value, 3))) %>% 
  rowwise() %>% 
  mutate(sig = getstars(p.value))

e3.tab.acc.gaze.use$term <- c("Intercept", "Total Exposure Correct", "Trial Type",
                              "Total Exposure Correct*Trial Type")

names(e3.tab.acc.gaze.use)[6] <- c("")

print(xtable(e3.tab.acc.gaze.use,
             align = c("l","l","r","r","r","r","l"),
             label = "tab:e3_acc_gaze_use"),
      include.rownames=FALSE,hline.after=c(0,nrow(e3.tab.acc.gaze.use)),
      sanitize.text.function=function(x){x},
      table.placement = "h",
      comment = F)
```

\newpage
\section*{Table A9. Accuracy on test trials in Experiment 3 as a function of each participants' subjective reliability judgment}
\texttt{Correct $\sim$ Trial Type * Subjective Reliability + offset(logit($^1/_{Referents}$)) + \\ (Trial Type | subject)}

```{r e3 acc test subj reliability, results = 'asis'}
# some code to clean up tables for paper
e3.tab.acc.subj.rel <- broom::tidy(m3_expt3) %>% 
  filter(group == "fixed") %>% 
  select(term:p.value) %>% 
  rename(z.value = statistic) %>% 
  mutate_at(.vars = c("estimate", "std.error", "z.value"), 
            .funs = round, digits = 2) %>% 
  mutate(p.value = ifelse(round(p.value, 3) == 0, "$<$ .001", round(p.value, 3))) %>% 
  rowwise() %>% 
  mutate(sig = getstars(p.value))

e3.tab.acc.subj.rel$term <- c("Intercept", "Subjective Reliability", "Trial Type",
                              "Subjective Reliability*Trial Type")

names(e3.tab.acc.subj.rel)[6] <- c("")

print(xtable(e3.tab.acc.subj.rel,
             align = c("l","l","r","r","r","r","l"),
             label = "tab:e3_acc_subj_rel"),
      include.rownames=FALSE,hline.after=c(0,nrow(e3.tab.acc.subj.rel)),
      sanitize.text.function=function(x){x},
      table.placement = "h",
      comment = F)
```

\section*{Table A10. Accuracy on test trials in Experiment 3 as a function of reliability condition and inspection time on exposure trials}
\texttt{Correct $\sim$ (Trial Type + Reliability condition + Trial Type + \\ Log(Inspection Time))$^2$ + offset(logit($^1/_{Referents}$)) + (Trial Type | subject)}

```{r e3 acc test inspection time table, results = 'asis'}
# some code to clean up tables for paper
e3.tab.acc.inspection <- broom::tidy(m4a_expt3_2) %>% 
  filter(group == "fixed") %>% 
  select(term:p.value) %>% 
  rename(z.value = statistic) %>% 
  mutate_at(.vars = c("estimate", "std.error", "z.value"), 
            .funs = round, digits = 2) %>% 
  mutate(p.value = ifelse(round(p.value, 3) == 0, "$<$ .001", round(p.value, 3))) %>% 
  rowwise() %>% 
  mutate(sig = getstars(p.value))

e3.tab.acc.inspection$term <- c("Intercept","Log(Inspection Time)", "Trial Type",
                                "Reliability Condition", "Log(Inspection Time)*Trial Type",
                                "Log(Inspection Time)*Reliability Condition", 
                                "Trial Type*Reliability Condition")

names(e3.tab.acc.inspection)[6] <- c("")

print(xtable(e3.tab.acc.inspection,
             align = c("l","l","r","r","r","r","l"),
             label = "tab:e3_acc_inspect"),
      include.rownames=FALSE,hline.after=c(0,nrow(e3.tab.acc.inspection)),
      sanitize.text.function=function(x){x},
      table.placement = "h",
      comment = F)
```

\newpage
\section*{Table A11. Accuracy on test trials in Experiment 4 as a function of gaze and interval}
\texttt{Correct $\sim$ (Trial Type + Gaze + Log(Interval))$^2$ + offset(logit($^1/_{Referents}$)) + \\ (Trial Type | subject)}

```{r e4 acc test inspection time table, results = 'asis'}
# some code to clean up tables for paper
e4.tab.acc <- broom::tidy(m1_acc_expt4) %>% 
  filter(group == "fixed") %>% 
  select(term:p.value) %>% 
  rename(z.value = statistic) %>% 
  mutate_at(.vars = c("estimate", "std.error", "z.value"), 
            .funs = round, digits = 2) %>% 
  mutate(p.value = ifelse(round(p.value, 3) == 0, "$<$ .001", round(p.value, 3))) %>% 
  rowwise() %>% 
  mutate(sig = getstars(p.value))

e4.tab.acc$term <- c("Intercept", "Trial Type", "Gaze Condition", "Log(Interval)",
                     "Trial Type*Gaze Condition", "Trial Type*Log(Interval)", 
                     "Gaze Condition*Log(Interval)")

names(e4.tab.acc)[6] <- c("")

print(xtable(e4.tab.acc,
             align = c("l","l","r","r","r","r","l"),
             label = "tab:e4_acc"),
      include.rownames=FALSE,hline.after=c(0,nrow(e3.tab.acc.inspection)),
      sanitize.text.function=function(x){x},
      table.placement = "h",
      comment = F)
```
