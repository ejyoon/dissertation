`r if(knitr:::is_latex_output()) '\\appendix'`

`r if(!knitr:::is_latex_output()) '# (APPENDIX) Appendix {-}'` 

<!--
If you feel it necessary to include an appendix, it goes here.
-->

```{r, include=FALSE}
knitr::opts_chunk$set(echo = F, cache = T, warning=F, message=F, 
                      fig.width=4, fig.asp = 0.618, out.width = "75%", 
                      fig.align = 'center', fig.pos = 't')
```

# Supplementary materials for Chapter 1

## Mathematical details of Optimal Experiment Design

This supplement contains the mathematical details of the OED approach as described in @coenen2017asking. The goal is to provide a concrete foundation for the conceptual analysis of how social learning contexts can influence different components of active learning. 

The OED model quantifies the *expected utility* of different information seeking actions. Formally, the set of queries is defined as $Q_1, Q_2,..., Q_n = \{Q\}$. The expected utility of each query ($EU(Q)$) is a function of two factors: (1) the probability of obtaining a specific answer $P(a)$ weighted by (2) the usefulness of that answer for achieving the learning goal $U(a)$.

$$EU(Q) = \sum_{a\in q}{P(a)U(a)}$$
\noindent
There are a variety of ways to define the usefulness function to score each answer. An exhaustive review is beyond the scope of this paper (for a detailed analysis of different approaches, see @nelson2005finding). One standard method is to use *information gain*, which is defined as the change in the learner's overall uncertainty (difference in entoropy) before and after receiving an answer.

$$U(a) = ent(H) - ent(H|a)$$

\noindent
Where $ent(H)$ is defined using Shannon entropy ^[Shannon entropy is a measure of unpredictability or amount of uncertainty in the learner's probability distribution over hypotheses. Intuitively, higher entropy distributions are more uncertain and harder to predict. For example, if the learner believes that all hypotheses are equally likely, then they are in a state of high uncertainty/entropy. In contrast, if the learner firmly believes in one hypothesis, then uncertainty/entropy is low.] [@mackay2003information], which provides a measure of the overall amount of uncertainty in the learner's beliefs about the candidate hypotheses. 

$$ent(H) = -\sum_{a\in A}{P(h)log_2P(h)}$$
\noindent
The conditional entropy computation is the same, but takes into account the change in the learner's beliefs after seeing an answer.

$$ ent(H|a) = -\sum_{h\in H}{P(h|a)logP(h|a)} $$
\noindent
To calculate the change in the learner's belief in a hypothesis $P(h|a)$, we use Bayes rule. 

$$ P(h|a) = \frac{P(h)P(a|h)}{P(a)} $$ 

\noindent
If the researcher defines all these parts of the OED model (hypotheses, questions, answers, and the usefulness function), then selecting the optimal query is straightforward. The learner performs the expected utility computation for each query in the set of possible queries and picks the one that maximizes utility. In practice, the learner considers each possible answer, scores the answer with the usefulness function, and weights the score using the probability of getting that answer. 

Before reviewing the behavioral evidence for OED-like reasoning in adults and children, I will present a worked example of how to compute the expected utility of a single query. The goal is to provide simple calculations that illustrate how reasoning about hypotheses, questions, and answers can lead to selecting useful actions. This example is slightly modified from @nelson2005finding. ^[In the [appendix](#app), I include example code for instantiating the OED calculations as functions in the R programming language.] 

Imagine that you are a biologist, and you come across a new animal that you think belongs to one of two species: "glom" or "fizo." You cannot directly query the category identity, but you can gather information about the presence or absence of two features (eats meat? or is nocturnal?) that you know from prior research are more or less likely for each of the species. The following probabilities summarise this prior knowledge:

  * $P(eatsMeat \mid glom) = 0.1$  
  * $P(eatsMeat \mid fizo) = 0.9$
  * $P(nocturnal \mid glom) = 0.3$  
  * $P(nocturnal \mid fizo) = 0.5$ 

\noindent  
You also know from previous research that the probability of seeing a glom or a fizo in the wild is:

  * $P(glom) = 0.7$ 
  * $P(fizo) = 0.3$ 

\noindent  
Which feature should you test: eats meat? or sleeps at night? Intuitively, it seems better to test whether the creature eats meat because an answer to this question provides good evidence about whether the animal is a fizo since $P(eatsMeat \mid fizo) = 0.9$. However, the OED computation allows the biologist to go beyond this intuition and compute precisely how much better it is to ask the "eats meat?" question. All the scientist has to do is pass her knowledge about the hypotheses and features through the expected utility computation. 

Here are the steps of the OED computation for calculating the utility of the "eats meat?" question. First, we use Bayes rule to calculate how much our beliefs would change if we received a "yes" or a "no" answer. ^[Note that the $P(eatsMeat)$ term is computed by taking $P(eatsMeat) = [P(eatsMeat \mid glom) \ times P(glom)] + [P(eatsMeat \mid fizo) \times P(fizo)] = (0.1 \times 0.7) + (0.9 \times 0.3) = 0.34$]

$$ P(glom \mid eatsMeat) = \frac{P(eatsMeat \mid glom) \times P(glom)}{P(eatsMeat)} = \frac{0.1 \times 0.7}{0.34} = 0.21 $$ 

\noindent
Next, we calculate the uncertainty over the Species hypothesis before doing any experiment. We do this by computing the prior entropy.

$$
\begin{aligned}
ent(Species) &= -\sum_{h\in H}{P(h) \times log_2P(h)} \\
 &= [-P(glom) \times log_2P(glom)]+[-P(fizo) \times log_2P(fizo)]\\
 &= [-(0.7 \times log_2(0.7)] + [-(0.3 \times log_2(0.3)]\\
 &= 0.88
\end{aligned}
$$

\noindent
To calculate information gain, we also need to compute our uncertainty over hypotheses conditional on seeing each answer, or the posterior entropy. First, for the "yes" answer:

$$
\begin{aligned}
ent(Species|eatsMeat = yes) &= -\sum_{a\in A}P(Species \mid eatsMeat = yes) \times log_2P(species \mid eatsMeat = yes) \\
&= [0.21 \times log_2(0.21)] + [0.79 \times log_2(0.79)]\\
&=  0.73
\end{aligned}
$$

\noindent
We use the difference between the prior and posterior entropy to compute the utility of the "yes" answer.

$$
\begin{aligned}
U(a = yes) &= ent(Species) - ent(Species \mid eatsMeat = yes)\\
&= 0.88 - 0.73 \\
&= 0.15
\end{aligned}
$$

\noindent
Next, we do the same process for the "no" answer. First, we calculate the posterior entropy.

$$ 
\begin{aligned}
ent(Species|eatsMeat = no) &= -\sum_{a\in A}{P(Species \mid eatsMeat = no) \times log_2P(species \mid eatsMeat = no)}\\
&= [0.95 \times log_2(0.95)] + [0.05 \times log_2(0.05)]\\
&=  0.27
\end{aligned}
$$

\noindent
Again, we use the difference between the prior and posterior entropy to compute the utility of the "no" answer.

$$ 
\begin{aligned}
U(a = no) &= ent(Species) - ent(Species \mid eatsMeat = no)\\
&= 0.88 - 0.27 \\
&= 0.61
\end{aligned}
$$

\noindent
Note that the $U(a = no) > U(a = yes)$. This captures the intuition that learning that the animal does not eat meat would provide strong evidence against the "fizo" hypothesis since $P(eatsMeat \mid fizo) = 0.9$. Finally, to compute the overall expected information gain for the **"eats meat?" question**, we weight the utility of each answer by its probability:

$$
\begin{aligned}
EU(Q = eatsMeat) &= \sum_{a\in A}{P(a)U(a)} \\
&= [P(eatsMeat = yes) \times U(eatsMeat = yes)] + \\& \qquad \qquad \qquad [P(eatsMeat = no) \times U(eatsMeat = no)]\\
&= [0.34 \times 0.15] + [0.66 \times 0.61]\\
&= 0.46
\end{aligned}
$$

If we performed the same steps to calculate the expected utility of the "sleeps at night?" question, we get $EU(Q = sleepsNight) = 0.026$. So if the biologist wants to maximize the chance of gaining useful information, she should select the "eats meat?" experiment since $EU(Q = eatsMeat) > EU(Q = sleepsNight)$.


# Supplementary materials for Chapter 2

In this appendix, we present four pieces of supplemental information. First, we provide details about the Bayesian models used to analyze the data. Second, we present a sensitivity analysis that provides evidence that the estimates of the associations between age/vocabulary and accuracy/reaction time (RT) are robust to different parameterizations of the prior distribution and different cutoffs for the analysis window. Third, we present the results of a parallel set of analyses using a non-Bayesian approach to show that these results are consistent regardless of choice of analytic framework. And fourth, we present two exploratory analyses measuring the effects of phonological overlap and iconicity on RT and accuracy. In both analyses, we did not see evidence that these factors changed the dynamics of eye movements during ASL processing

## Model Specifications

Our key analyses use Bayesian linear models to test our hypotheses of interest and to estimate the associations between age/vocabulary and RT/accuracy. Figure S1 (Accuracy) and S2 (RT) present graphical models that represent all of the data, parameters, and other variables of interest, and their dependencies. Latent parameters are shown as unshaded nodes while observed parameters and data are shown as shaded nodes. All models were fit using JAGS software (Plummer, 2003) and adapted from models in Kruschke (2014) and Lee and Wagenmakers (2014).

### Accuracy

```{r, fig.cap = "Graphical model representation of the linear regression used to predict accuracy. The shaded nodes represent observed data (i.e., each participant's age, vocabulary, and mean accuracy). Unshaded nodes represent latent parameters (i.e., the intercept and slope of the linear model).", fig.scap="Graphical representation of the accuracy model."}

include_graphics(path = here::here(data_code_path_sol, "figures/acc_model.png"))
```

To test the association between age/vocabulary and accuracy we assume each participant's mean accuracy is drawn from a Gaussian distribution with a mean, $\mu$, and a standard deviation, $\sigma$. The mean is a linear function of the intercept, $\alpha$, which encodes the expected value of the outcome variable when the predictor is zero, and the slope, $\beta$, which encodes the expected change in the outcome with each unit change in the predictor (i.e., the strength of association). 


For $\alpha$ and $\sigma$, we use vague priors on a standardized scale, allowing the model to consider a wide range of plausible values. Since the slope parameter $\beta$ is critical to our hypothesis of a linear association, we chose to use an informed prior: that is, a truncated Gaussian distribution with a mean of zero and a standard deviation of one on a standardized scale. Centering the distribution at zero is conservative and places the highest prior probability on a null association, to reduce the chance that our model overfits the data. Truncating the prior encodes our directional hypothesis that accuracy should increase with age and larger vocabulary size. And using a standard deviation of one constrains the plausible slope values, thus making our alternative hypothesis more precise. We constrained the slope values based on previous research with children learning spoken language showing that the average gain in accuracy for one month of development between 18-24 months to be ~1.5%  (Fernald, Zangl, Portillo, & Marchman, 2008). 

### Reaction Time

```{r, fig.cap = "Graphical model representation of the linear regression plus latent mixture model (i.e., guessing model). The model assumes that each individual participant's first shift is either the result of guessing or knowledge. And the latent indicator $z_i$ determines whether that participant is included in the linear regression estimating the association between age/vocabulary and RT.", fig.scap="Graphical representation of the RT model."}

include_graphics(path = here::here(data_code_path_sol, "figures/rt_model.png"))
```

The use of RT as a processing measure is based on the assumption that the timing of a child's first shift reflects the speed of their incremental language comprehension. Yet, some children have a first shift that seems to be unassociated with this construct: their first shift behavior appears random. We quantify this possibility for each participant explicitly (i.e., the probability that the participant is a "guesser") and we create an analysis model where participants who were more likely to be guessers have less of an influence on the estimated relations between RT and age/vocabulary.

To quantify each participant's probability of guessing, we computed the proportion of signer-to-target (correct) and signer-to-distracter (incorrect) shifts for each child. We then used a latent mixture model in which we assumed that the observed data, k_i, were generated by two processes (guessing and knowledge) that had different overall probabilities of success, with the "guessing group" having a probability of 50%, $\psi$, and the "knowledge" group having a probability greater than 50%, $\phi$. The group membership of each participant is a latent indicator variable, $z_i$, inferred based on that participant's proportion of correct signer-to-target shifts relative to the overall proportion of correct shifts across all participants (see Lee & Wagenmakers (2014) for a detailed discussion of this modeling approach). We then used each participant's inferred group membership to determine whether they were included in the linear regression. In sum, the model allows participants to contribute to the estimated associations between age/vocabulary and RT proportional to our belief that they were guessing.

As in the Accuracy model, we use vague priors for $\alpha$ and $\sigma$ on a standardized scale. We again use an informed prior for $\beta$, making our alternative hypothesis more precise. That is, we constrained the plausible slope values based on previous research with children learning spoken language showing that the average gain in RT for one month of development between 18-24 months to be ~30 ms (Fernald, Zangl, Portillo, & Marchman, 2008).

## Sensitivity Analysis: Prior Distribution and Window Selection

```{r, fig.cap = "Coefficient plot for the slope parameter for four different parameterizations of the prior and for three different analysis windows. Each panel shows a different model. Each point represents a coefficient measuring the strength of association between the two variables. Error bars are 95\\% HDIs around the coefficient. Color represents the three different analysis windows.", fig.scap="Results of sensitivity analysis for Experiment 1.1."}

include_graphics(path = here::here(data_code_path_sol, "figures/supp_coef_plot.png"))
```

We conducted a sensitivity analysis to show that our parameter estimates for the associations between accuracy/RT and age/vocabulary are robust to decisions about (a) the analysis window and (b) the specification of the prior distribution on the slope parameter. Specifically, we varied the parameterization of the standard deviation on the slope, allowing the model to consider a wider or narrower range of values to be plausible a priori. We also fit these different models to two additional analysis windows +/- 300 ms from the final analysis window: 600-2500 ms (the middle 90% of the RT distribution in our experiment).

```{r}
d_sensitivity <- read_delim(here::here(data_code_path_sol, "data/processed_data/sol_supp_sensitivity_table.txt"), delim = "\t")

d_sensitivity %>% 
  kable("latex", 
        booktabs = T, 
        caption = "Bayes Factors for all four linear models fit to three different analysis windows using four different parameterizations of the prior distribution for the slope parameter.",
        caption.short = "Results for sensitivity analysis for Experiment 1.1") %>%
  kable_styling(latex_options = "scale_down") %>% 
  row_spec(row = 0, bold = TRUE) %>% 
  column_spec(column = 2, width = "3cm") %>% 
  column_spec(column = 1, width = "3.5cm") 
```


Figure S3 shows the results of the sensitivity analysis, plotting the coefficient for the $\beta$ parameter in each model for the three different analysis windows for each specification of the prior. All models show similar coefficient values, suggesting that inferences about the parameters are not sensitive to the exact form of the priors. Table S1 shows the Bayes Factors for all models across three analysis windows and fit using four different vales for the slope prior. The Bayes Factor only drops below 3 when the prior distribution is quite broad (standard deviation of 3.2) and only for the longest analysis window (600-2800 ms). In sum, the strength of evidence for a linear association is robust to the choice of analysis window and prior specification.

## Parallel set of non-Bayesian analyses

First, we compare Accuracy and RT of native hearing and deaf signers using a Welch Two Sample t-test and do not find evidence that these groups are different (Accuracy: t(28) = 0.75, p = 0.45, 95% CI on the difference in means [-0.07, 0.14]; RT: t(28) = 0.75, p = 0.46, 95% CI on the difference in means [-125.47 ms, 264.99 ms]. 

Second, we test whether children and adults tend to generate saccades away from the central signer prior to the offset of the target sign. To do this, we use a One Sample t-test with a null hypothesis that the true mean is not equal to 1, and we find evidence against this null (Children: M = 0.88, t(28) = -2.92, p = 0.007, 95% CI [0.79, 0.96]; Adults: M = 0.51, t(15) = -6.87, p < 0.001, 95% CI [0.35, 0.65])

Third, we fit the four linear models using MLE to estimate the relations between the processing measures on the VLP task (Accuracy/RT) and age/vocabulary. We follow recommendations from Barr (2008) and use a logistic transform to convert the proportion accuracy scores to a scale more suitable for the linear model. Table XXX shows the results. 

```{r}
d_lm <- read_delim(here::here(data_code_path_sol, "data/processed_data/sol_supp_freq_table.txt"), delim = "\t")

d_lm %>% 
  kable("latex", 
        booktabs = T, 
        caption = "Results for the four linear models fit using Maxiumum Likelihood Estimation. All p-values are one-sided to reflect our directional hypotheses about the VLP measures improving over development.",
        caption.short = "Results for MLE models") %>%
  kable_styling(latex_options = "scale_down") %>% 
  row_spec(row = 0, bold = TRUE) %>% 
  column_spec(column = 2, width = "3cm") %>% 
  column_spec(column = 1, width = "3.5cm") 
```

## Analyses of phonological overlap and iconicity

```{r, fig.cap = "Scatterplot of the association between degree of phonological overlap and RT (top row) and accuracy (bottom row) for both adults (left column) and children (right column). The blue line represents a linear model fit.", fig.scap="association between degree of phonological overlap and RT/Accuracy."}

include_graphics(path = here::here(data_code_path_sol, "figures/phonological_overlap.png"))
```

First, we analyzed whether phonological overlap of our item-pairs might have influenced adults and children’s RTs and accuracy. Signs that are higher in phonological overlap might have been more difficult to process because they are more confusable. Here, phonological overlap is quantified as the number of features (e.g., Selected Fingers, Major Location, Movement, Sign Type) that both signs shared. Values were taken from a recently created database (ASL-LEX) of lexical and phonological properties of nearly 1,000 signs of American Sign Language (Caselli et al., 2017). Our item-pairs varied in degree of overlap from 1-4 features.  We did not see evidence that degree of phonological overlap influenced either processing measure in the VLP task.

```{r, fig.cap = "Scatterplot of the association between degree of iconicity and RT (top row) and accuracy (bottom row) for both adults (left column) and children (right column). The blue line represents a linear model fit.", fig.scap="association between degree of iconicity and RT/Accuracy."}

include_graphics(path = here::here(data_code_path_sol, "figures/iconicity.png"))
```


Next, we performed a parallel analysis, exploring whether the iconicity of our signs might have influenced adults and children’s RT and accuracy. It is possible that highly iconic signs might be easier to process because of the visual similarity to the target object. Again, we used ASL-LEX to quantify the iconicity of our signs. To generate these values, native signers were asked to explicitly rate the iconicity of each sign on a scale of 1-7, with 1 being not iconic at all and 7 being very iconic. Similar to the phonological overlap analysis, we did see evidence that degree of iconicity influenced either processing measure for either age group in the VLP task.

# Supplementary materials for Chapter 4

## Analytic model specifications and output

### Experiment 1

\captionsetup[table]{labelformat=empty}

\section*{Table A1. Length of inspection times on exposure trials in Experiment 1 as a function of gaze, interval, and number of referents}
\texttt{Log(Inspection time) $\sim$ (Gaze + Log(Interval) + Log(Referents))$^2$ + (1 | subject)}

```{r e1 inspect model table, results = 'asis'}
# some code to clean up tables for paper
e1.tab.inspect <- broom::tidy(m1_rt_expt1) %>% 
  filter(group == "fixed") %>% 
  select(term:statistic) %>% 
  rename(t.value = statistic) %>% 
  mutate(p.value = 2 * (1 - pnorm(abs(t.value)))) %>% 
  mutate_at(.vars = c("estimate", "std.error", "t.value"), 
            .funs = round, digits = 2) %>% 
  mutate(p.value = ifelse(round(p.value, 3) == 0, "$<$ .001", round(p.value, 3))) %>% 
  rowwise() %>% 
  mutate(sig = getstars(p.value))

e1.tab.inspect$term <- c("Intercept","Gaze Condition","Log(Interval)",
                         "Log(Referents)","Gaze Condition*Log(Interval)", 
                         "Gaze Condition*Log(Referent)", "Log(Interval)*Log(Referent)")

names(e1.tab.inspect)[6] <- c("")

print(xtable(e1.tab.inspect,
             align = c("l","l","r","r","r","r","l"),
             label = "tab:e1_rt"),
      include.rownames=FALSE, 
      hline.after=c(0,nrow(e1.tab.inspect)),
      sanitize.text.function=function(x){x},
      table.placement = "h",
      comment = F)
```

\newpage
\section*{Table A2. Accuracy on test trials in Experiment 1 with inspection times on exposure trials included as a predictor}
\texttt{Correct $\sim$ (Trial Type + Gaze + Log(Interval) + Log(Referents) + \\ Log(Inspection Time))$^2$ + offset(logit($^1/_{Referents}$)) + (TrialType | subject)}

```{r e1 inspect model acc table, results = 'asis'}
# some code to clean up tables for paper
e1.tab.inspect.acc <- broom::tidy(m1_2way_acc_expt1_inspect) %>% 
  filter(group == "fixed") %>% 
  select(term:p.value) %>% 
  rename(z.value = statistic) %>% 
  mutate_at(.vars = c("estimate", "std.error", "z.value"), 
            .funs = round, digits = 2) %>% 
  mutate(p.value = ifelse(round(p.value, 3) == 0, "$<$ .001", round(p.value, 3))) %>% 
  rowwise() %>% 
  mutate(sig = getstars(p.value))

e1.tab.inspect.acc$term <- c("Intercept", "Switch Trial", "Gaze Condition","Log(Interval)",
                             "Log(Referents)","Log(Inspection Time)","Switch Trial*Gaze Condition",
                             "Switch Trial*Log(Interval)", "Switch Trial*Log(Referent)",
                             "Switch Trial*Log(Inspection Time)", "Gaze Condition*Log(Interval)",
                             "Gaze Condition*Log(Referent)", "Gaze Condition*Log(Inspection Time)",
                             "Log(Interval)*Log(Referent)", "Log(Interval)*Log(Inspection Time)",
                             "Log(Referents)*Log(Inspection Time)")

names(e1.tab.inspect.acc)[6] <- c("")

print(xtable(e1.tab.inspect.acc,
             align = c("l","l","r","r","r","r","l"),
             label = "tab:e1_acc_it"),
      include.rownames=FALSE,hline.after=c(0,nrow(e1.tab.inspect.acc)),
      sanitize.text.function=function(x){x},
      table.placement = "h",
      comment = F)
```

\newpage

### Experiment 2

\section*{Table A3. Length of inspection times on exposure trials in Experiment 2 as a function of gaze and interval}
\texttt{Log(Inspection time) $\sim$ Gaze * Log(Interval) + (1 | subject)}

```{r e2 inspect model table, results = 'asis'}
# some code to clean up tables for paper
e2.tab.inspect <- broom::tidy(m1_rt_expt2) %>% 
  filter(group == "fixed") %>% 
  select(term:statistic) %>% 
  rename(t.value = statistic) %>% 
  mutate(p.value = 2 * (1 - pnorm(abs(t.value)))) %>% 
  mutate_at(.vars = c("estimate", "std.error", "t.value"), 
            .funs = round, digits = 2) %>% 
  mutate(p.value = ifelse(round(p.value, 3) == 0, "$<$ .001", round(p.value, 3))) %>% 
  rowwise() %>% 
  mutate(sig = getstars(p.value))

e2.tab.inspect$term <- c("Intercept", "Gaze Condition", "Log(Interval)",
                         "Gaze Condition*Log(Interval)")

names(e2.tab.inspect)[6] <- c("")

print(xtable(e2.tab.inspect,
             align = c("l","l","r","r","r","r","l"),
             label = "tab:e2_rt"),
      include.rownames=FALSE,hline.after=c(0,nrow(e2.tab.inspect)),
      sanitize.text.function=function(x){x},
      table.placement = "h",
      comment = F)
```

\section*{Table A4. Accuracy on test trials in Experiment 2 with inspection times on exposure trials included as a predictor}
\texttt{Correct $\sim$ (Trial Type + Gaze + Log(Interval) + Log(Inspection Time))$^2$ + \\ offset(logit($^1/_{Referents}$)) + (TrialType | subject)}

```{r e2 inspect model acc table, results = 'asis'}
# some code to clean up tables for paper
e2.tab.inspect.acc <- broom::tidy(m2_inspect_e2) %>% 
  filter(group == "fixed") %>% 
  select(term:p.value) %>% 
  rename(z.value = statistic) %>% 
  mutate_at(.vars = c("estimate", "std.error", "z.value"), 
            .funs = round, digits = 2) %>% 
  mutate(p.value = ifelse(round(p.value, 3) == 0, "$<$ .001", round(p.value, 3))) %>% 
  rowwise() %>% 
  mutate(sig = getstars(p.value))

e2.tab.inspect.acc$term <- c("Intercept", "Gaze Condition", "Switch Trial", "Log(Interval)",
                             "Log(Inspection Time)", "Switch Trial*Gaze Condition",
                             "Gaze Condition*Log(Interval)", "Gaze Condition*Log(Inspection Time)",
                             "Switch Trial*Log(Interval)", "Switch Trial*Log(Inspection Time)", 
                             "Log(Interval)*Log(Inspection Time)")

names(e2.tab.inspect.acc)[6] <- c("")

print(xtable(e2.tab.inspect.acc,
             align = c("l","l","r","r","r","r","l"),
             label = "tab:e2_acc_it"),
      include.rownames=FALSE,hline.after=c(0,nrow(e2.tab.inspect.acc)),
      sanitize.text.function=function(x){x},
      table.placement = "h",
      comment = F)
```

\newpage

### Experiment 3

\section*{Table A5. Accuracy on exposure trials in Experiment 3 as a function of reliability condition and participants' subjective reliability judgments}
\texttt{Correct-Exposure $\sim$ Reliability Condition * Subjective Reliability + \\  offset(logit($^1/_{Referents}$)) + (1 | subject)}

```{r e3 gf on exposure table, results = 'asis'}
# some code to clean up tables for paper
e3.tab.gf.exposure <- broom::tidy(m1_expo_expt3) %>% 
filter(group == "fixed") %>% 
select(term:p.value) %>% 
rename(z.value = statistic) %>% 
mutate_at(.vars = c("estimate", "std.error", "z.value"), 
.funs = round, digits = 2) %>% 
mutate(p.value = ifelse(round(p.value, 3) == 0, "$<$ .001", round(p.value, 3))) %>% 
rowwise() %>% 
mutate(sig = getstars(p.value))

e3.tab.gf.exposure$term <- c("Intercept", "Reliability Condition", "Subjective Reliability",
"Reliability Condition*Subjective Reliability")

names(e3.tab.gf.exposure)[6] <- c("")

print(xtable(e3.tab.gf.exposure,
align = c("l","l","r","r","r","r","l"),
label = "tab:e3_gf_exp"),
include.rownames=FALSE,hline.after=c(0,nrow(e3.tab.gf.exposure)),
sanitize.text.function=function(x){x},
table.placement = "h",
comment = F)
```

\section*{Table A6. Accuracy on test trials in Experiment 3 as a function of reliability condition}
\texttt{Correct $\sim$ Trial Type * Reliability Condition + offset(logit($^1/_{Referents}$)) + \\ (Trial Type | subject)}

```{r e3 acc test rel cond table, results = 'asis'}
# some code to clean up tables for paper
e3.tab.acc.rel.cond <- broom::tidy(m1_expt3) %>% 
filter(group == "fixed") %>% 
select(term:p.value) %>% 
rename(z.value = statistic) %>% 
mutate_at(.vars = c("estimate", "std.error", "z.value"), 
.funs = round, digits = 2) %>% 
mutate(p.value = ifelse(round(p.value, 3) == 0, "$<$ .001", round(p.value, 3))) %>% 
rowwise() %>% 
mutate(sig = getstars(p.value))

e3.tab.acc.rel.cond$term <- c("Intercept", "Trial Type", "Reliability Condition",
"Reliability Condition*Trial Type")

names(e3.tab.acc.rel.cond)[6] <- c("")

print(xtable(e3.tab.acc.rel.cond,
align = c("l","l","r","r","r","r","l"),
label = "tab:e3_acc_rel_cond"),
include.rownames=FALSE,hline.after=c(0,nrow(e3.tab.acc.rel.cond)),
sanitize.text.function=function(x){x},
table.placement = "h",
comment = F)
```

\newpage
\section*{Table A7. Accuracy on test trials in Experiment 3 as a function of reliability condition and participants' use of gaze on exposure trials}
\texttt{Correct $\sim$ (Trial Type + Reliability Condition + Correct-Exposure)$^2$ \\ + offset(logit($^1/_{Referents}$)) + (Trial Type | subject)}

```{r e3 acc test rel and gaze follow table, results = 'asis'}
# some code to clean up tables for paper
e3.tab.acc.rel.cond.gf <- broom::tidy(m2a_expt3) %>% 
  filter(group == "fixed") %>% 
  select(term:p.value) %>% 
  rename(z.value = statistic) %>% 
  mutate_at(.vars = c("estimate", "std.error", "z.value"), 
            .funs = round, digits = 2) %>% 
  mutate(p.value = ifelse(round(p.value, 3) == 0, "$<$ .001", round(p.value, 3))) %>% 
  rowwise() %>% 
  mutate(sig = getstars(p.value))

e3.tab.acc.rel.cond.gf$term <- c("Intercept", "Correct Exposure", "Trial Type", "Reliability Condition",
                                 "Correct Exposure*Trial Type", "Correct Exposure*Reliability",
                                 "Reliability Condition*Trial Type")

names(e3.tab.acc.rel.cond.gf)[6] <- c("")

print(xtable(e3.tab.acc.rel.cond.gf,
             align = c("l","l","r","r","r","r","l"),
             label = "tab:e3_acc_rel_cond_gf"),
      include.rownames=FALSE,hline.after=c(0,nrow(e3.tab.acc.rel.cond.gf)),
      sanitize.text.function=function(x){x},
      table.placement = "h",
      comment = F)
```

\section*{Table A8. Accuracy on test trials in Experiment 3 as a function of each participants' accuracy on exposure trials}
\texttt{Correct $\sim$ Trial Type * Total Correct Exposure + offset(logit($^1/_{Referents}$)) + \\ (Trial Type | subject)}

```{r e3 acc test gaze use table, results = 'asis'}
# some code to clean up tables for paper
e3.tab.acc.gaze.use <- broom::tidy(m2b_expt3) %>% 
  filter(group == "fixed") %>% 
  select(term:p.value) %>% 
  rename(z.value = statistic) %>% 
  mutate_at(.vars = c("estimate", "std.error", "z.value"), 
            .funs = round, digits = 2) %>% 
  mutate(p.value = ifelse(round(p.value, 3) == 0, "$<$ .001", round(p.value, 3))) %>% 
  rowwise() %>% 
  mutate(sig = getstars(p.value))

e3.tab.acc.gaze.use$term <- c("Intercept", "Total Exposure Correct", "Trial Type",
                              "Total Exposure Correct*Trial Type")

names(e3.tab.acc.gaze.use)[6] <- c("")

print(xtable(e3.tab.acc.gaze.use,
             align = c("l","l","r","r","r","r","l"),
             label = "tab:e3_acc_gaze_use"),
      include.rownames=FALSE,hline.after=c(0,nrow(e3.tab.acc.gaze.use)),
      sanitize.text.function=function(x){x},
      table.placement = "h",
      comment = F)
```

\newpage
\section*{Table A9. Accuracy on test trials in Experiment 3 as a function of each participants' subjective reliability judgment}
\texttt{Correct $\sim$ Trial Type * Subjective Reliability + offset(logit($^1/_{Referents}$)) + \\ (Trial Type | subject)}

```{r e3 acc test subj reliability, results = 'asis'}
# some code to clean up tables for paper
e3.tab.acc.subj.rel <- broom::tidy(m3_expt3) %>% 
  filter(group == "fixed") %>% 
  select(term:p.value) %>% 
  rename(z.value = statistic) %>% 
  mutate_at(.vars = c("estimate", "std.error", "z.value"), 
            .funs = round, digits = 2) %>% 
  mutate(p.value = ifelse(round(p.value, 3) == 0, "$<$ .001", round(p.value, 3))) %>% 
  rowwise() %>% 
  mutate(sig = getstars(p.value))

e3.tab.acc.subj.rel$term <- c("Intercept", "Subjective Reliability", "Trial Type",
                              "Subjective Reliability*Trial Type")

names(e3.tab.acc.subj.rel)[6] <- c("")

print(xtable(e3.tab.acc.subj.rel,
             align = c("l","l","r","r","r","r","l"),
             label = "tab:e3_acc_subj_rel"),
      include.rownames=FALSE,hline.after=c(0,nrow(e3.tab.acc.subj.rel)),
      sanitize.text.function=function(x){x},
      table.placement = "h",
      comment = F)
```

\section*{Table A10. Accuracy on test trials in Experiment 3 as a function of reliability condition and inspection time on exposure trials}
\texttt{Correct $\sim$ (Trial Type + Reliability condition + Trial Type + \\ Log(Inspection Time))$^2$ + offset(logit($^1/_{Referents}$)) + (Trial Type | subject)}

```{r e3 acc test inspection time table, results = 'asis'}
# some code to clean up tables for paper
e3.tab.acc.inspection <- broom::tidy(m4a_expt3_2) %>% 
  filter(group == "fixed") %>% 
  select(term:p.value) %>% 
  rename(z.value = statistic) %>% 
  mutate_at(.vars = c("estimate", "std.error", "z.value"), 
            .funs = round, digits = 2) %>% 
  mutate(p.value = ifelse(round(p.value, 3) == 0, "$<$ .001", round(p.value, 3))) %>% 
  rowwise() %>% 
  mutate(sig = getstars(p.value))

e3.tab.acc.inspection$term <- c("Intercept","Log(Inspection Time)", "Trial Type",
                                "Reliability Condition", "Log(Inspection Time)*Trial Type",
                                "Log(Inspection Time)*Reliability Condition", 
                                "Trial Type*Reliability Condition")

names(e3.tab.acc.inspection)[6] <- c("")

print(xtable(e3.tab.acc.inspection,
             align = c("l","l","r","r","r","r","l"),
             label = "tab:e3_acc_inspect"),
      include.rownames=FALSE,hline.after=c(0,nrow(e3.tab.acc.inspection)),
      sanitize.text.function=function(x){x},
      table.placement = "h",
      comment = F)
```

\newpage

### Experiment 4

\section*{Table A11. Accuracy on test trials in Experiment 4 as a function of gaze and interval}
\texttt{Correct $\sim$ (Trial Type + Gaze + Log(Interval))$^2$ + offset(logit($^1/_{Referents}$)) + \\ (Trial Type | subject)}

```{r e4 acc test inspection time table, results = 'asis'}
# some code to clean up tables for paper
e4.tab.acc <- broom::tidy(m1_acc_expt4) %>% 
  filter(group == "fixed") %>% 
  select(term:p.value) %>% 
  rename(z.value = statistic) %>% 
  mutate_at(.vars = c("estimate", "std.error", "z.value"), 
            .funs = round, digits = 2) %>% 
  mutate(p.value = ifelse(round(p.value, 3) == 0, "$<$ .001", round(p.value, 3))) %>% 
  rowwise() %>% 
  mutate(sig = getstars(p.value))

e4.tab.acc$term <- c("Intercept", "Trial Type", "Gaze Condition", "Log(Interval)",
                     "Trial Type*Gaze Condition", "Trial Type*Log(Interval)", 
                     "Gaze Condition*Log(Interval)")

names(e4.tab.acc)[6] <- c("")

print(xtable(e4.tab.acc,
             align = c("l","l","r","r","r","r","l"),
             label = "tab:e4_acc"),
      include.rownames=FALSE,hline.after=c(0,nrow(e3.tab.acc.inspection)),
      sanitize.text.function=function(x){x},
      table.placement = "h",
      comment = F)
```


