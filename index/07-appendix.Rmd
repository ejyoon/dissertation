`r if(knitr:::is_latex_output()) '\\appendix'`

`r if(!knitr:::is_latex_output()) '# (APPENDIX) Appendix {-}'` 

<!--
If you feel it necessary to include an appendix, it goes here.
-->

```{r, include=FALSE}
knitr::opts_chunk$set(echo = F, cache = T, warning=F, message=F, 
                      fig.width=4, fig.asp = 0.618, out.width = "75%", 
                      fig.align = 'center', fig.pos = 't')
```


# Supplementary materials for Chapter 3

## Stimuli

### Training trial

Nicole gave her friend a gift. Was Nicole nice? Was Nicole mean?

James hit his friend.
Was James nice? Was James mean?

Kyle broke his mom’s vase, and he told his mom the truth that he broke it. Was Kyle telling the truth?

Pam ate five cookies, but Pam told her mom a lie that she didn’t eat any cookie. Was Pam telling the truth?

### Example test story (experimental condition)

Look, this is Edward!
One day, Edward decided to bake some cookies.
Edward brought his cookie to school and met his friend Sally. Edward said to his friend Sally, “Here, try my cookie!”
Sally tasted the cookie,
and she did not like the cookie at all — she thought the cookie tasted yucky!
So did Sally like the cookie or did she not like the cookie?

Edward asked Sally, “Sally, how did you like my cookie?” Sally told Edward, “Edward, your cookie was tasty.”
So what did Sally tell Edward again?

Let’s think about the story again.
Sally thought the cookie was yucky.
And Sally told Edward that the cookie was yucky.
Was Sally nice? Was Sally mean? Was Sally telling the truth?

Look, this is Edward again!
One day, Edward decided to bake some cookies.
Edward brought his cookie to school and met his friend Mary. Edward said to his friend Mary, “Here, try my cookie!”
Mary tasted the cookie,
and she did not like the cookie at all — she thought the cookie tasted yucky!
So did Mary like the cookie or did she not like the cookie?

Edward asked Mary, “Mary, how did you like my cookie?” Mary told Edward, “Edward, your cookie was tasty.”
So what did Mary tell Edward again?

Let’s think about the story again.
Mary thought the cookie was yucky.
And Mary told Edward that the cookie was tasty.
Was Mary nice? Was Mary mean? Was Mary telling the truth?

Remember Sally and Mary from our story? Look, here they are.
Remember, Sally thought the cookie was yucky and told Edward that the cookie was yucky. 
Mary thought the cookie was yucky and told Edward that the cookie was tasty.
Who do you want to play with more, Sally or Mary? Why?

### Example test story (control condition)

Look, this is Sally!
One day, Sally saw a free cookie.
Sally said, “It’s a free cookie, I’ll try it!”
Sally tasted the cookie,
and she did not like the cookie at all — she thought the cookie tasted yucky!
So did Sally like the cookie or did she not like the cookie?

Sally’s friend Edward also wanted to taste the cookie. Edward asked Mary, “Sally, how did you like the cookie?” 
Sally told Edward, “Edward, the cookie was yucky.”
So what did Sally tell Edward again?

Let’s think about the story again.
Sally thought the cookie was yucky.
And Sally told Edward that the cookie was yucky
Was Sally nice? Was Sally mean? Was Sally telling the truth?

Look, this is Mary!
One day, Mary saw a free cookie.
Mary said, “It’s a free cookie, I’ll try it!”
Mary tasted the cookie,
and she did not like the cookie at all — she thought the cookie tasted yucky!
So did Mary like the cookie or did she not like the cookie?

Mary’s friend Edward also wanted to taste the cookie. Edward asked Mary, “Mary, how did you like the cookie?” 
Mary told Edward, “Edward, the cookie was tasty.”
So what did Mary tell Edward again?

Let’s think about the story again.
Mary thought the cookie was yucky.
And Mary told Edward that the cookie was tasty.
Was Mary nice? Was Mary mean? Was Mary telling the truth?

Remember Sally and Mary from our story? Look, here they are.
Remember, Sally thought the cookie was yucky and told Edward that the cookie was yucky. 
Mary thought the cookie was yucky and told Edward that the cookie was tasty.
Who do you want to play with more, Sally or Mary? Why?

## Supplemental figure

```{r}
trupol_appendix <- d_plot %>%
  ggplot(., aes(x=age, y=answer, col=cond)) +
  geom_jitter(height=.05, width=.05) +
  geom_smooth(span=2) +
  facet_grid(qkind~speaker) + 
  theme_few() +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  geom_hline(aes(yintercept=0.5), linetype="dashed") +
  scale_colour_ptol() + 
  # scale_alpha_manual(values=c(0,1)) +
  # ggtitle("Expt 1: Judgments for honest vs. polite speaker") +
  ylab("Proportion \"yes [the speaker was _____]\"") +
  xlab("Age") +
  ylim(-0.1,1.1) +
  scale_y_continuous(breaks=seq(0,1,.5)) +
  guides(size=FALSE)

ggsave(trupol_appendix, file=here::here(child_lies_image_path, "trupol_plot_appendix.png"), width=5, height=5)
```

```{r figTrupolAppendix, fig.env = "figure*", fig.pos = "p", fig.width=7, fig.height=5, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Speaker judgments by participants of different age (x-axis) for the honest speaker (left column) and the dishonest speaker (right), in different contexts (colors). Rows represent question types (e.g., Was Sally telling the truth?), and y-axis represents proportion saying \`\`yes\" to the question. Each point represents a participant response in a trial.", fig.scap="Speaker ratings for the experiment in Chapter 4 with age as a continuous variable."}
png::readPNG(here::here(child_lies_image_path, "trupol_plot_appendix.png")) %>%
  grid::grid.raster()
```


# Supplementary materials for Chapter 4

## Model details

The *literal listener* $L_0$ is a simple Bayesian agent that takes the utterance to be true:

$$P_{L_0}(s | w) \propto [\![ w ]\!] (s) * P(s).$$

\noindent where $[\![ w ]\!](s)$ is the truth-functional denotation of the utterance
$w$ (i.e. the utterance's literal meaning): It is a function
that maps world-states $s$ to Boolean truth values. The literal
meaning is used to update the literal listener's prior beliefs
over world states $P(s)$.

The *speaker* $S_1$ chooses utterances approximately optimally given a utility function, which can be decomposed into two components. 
First, informational utility ($U_{inf}$) is the amount of information a literal listener $L_0$ would still not know about world state $s$ after hearing a speaker's utterance $w$. 
Second, social utility ($U_{soc}$) is the expected subjective utility of the state inferred given the utterance $w$. 
The utility of an utterance subtracts the cost $c(w)$ from the weighted combination of the social and epistemic utilities. 

$$U(w; s; \phi_{S_1}) = \phi_{S_1} \cdot \ln(P_{L_0}(s \mid w)) + (1 - \phi_{S_1}) \cdot \mathbb{E}_{P_{L_0}(s \mid w)}[V(s)] - C(w).$$

\noindent The speaker then chooses utterances $w$ softmax-optimally given the state $s$ and his goal weight mixture $\phi_{S_1}$: 

$$P_{S_1}(w \mid s, \phi_{S_1}) \propto \mathrm{exp}(\lambda_{1} \cdot \mathbb{E}[U(w; s; \phi_{S_1})]).$$

## Literal semantic task

We probed judgments of literal meanings of the target words assumed by our model and used in our main experiment. 

### Participants 

51 participants with IP addresses in the United States were recruited on Amazon’s Mechanical Turk. 

### Design and Methods

We used thirteen different context items in which a speaker evaluated a performance of some kind. 
For example, in one of the contexts, Ann saw a presentation, and Ann’s feelings toward the presentation (true state) were shown on a scale from zero to three hearts (e.g., two out of three hearts filled in red color; see Figure\ \ref{fig:screenshot} for an example of the heart scale). 
The question of interest was "Do you think Ann thought the presentation was / wasn’t X?" and participants responded by choosing either “no” or “yes.” 
The target could be one of four possible words: *terrible*, *bad*, *good*, and *amazing*, giving rise to eight different possible utterances (with negation or no negation). 
Each participant read 32 scenarios, depicting every possible combination of states and utterances. 
The order of context items was randomized, and there were a maximum of four repeats of each context item per participant. 

### Behavioral results

We analyzed the data by collapsing across context items.
For each utterance-state pair, we computed the posterior distribution over the semantic weight (i.e., how consistent X utterance is with Y state) assuming a uniform prior over the weight (i.e., a standard Beta-Binomial model). 
Meanings of the words as judged by participants were as one would expect (Figure\ \ref{fig:litsem}). 

```{r litSem, echo=FALSE}
d <- read.csv(here::here(file_path, "literal_semantics.csv")) %>%
  mutate(utterance = fct_relevel(utterance, "terrible", "bad", "good", "amazing"))

ms <- d %>%
  mutate(positivity = fct_recode(positivity,
                                "it was ~ " = "it was ___",
                                "it wasn't ~ " = "it wasn't ___"
                                ),
         positivity = fct_relevel(positivity, "it was ~ ")) %>%
  group_by(positivity, state, utterance, subid) %>%
  summarize(
            judgment = mean(judgment, na.rm=TRUE)
          ) %>%
  group_by(positivity, state, utterance) %>%
  multi_boot_standard(col = "judgment") %>%
  mutate(judgment = mean)

litsem_plot <- qplot(state, judgment, 
      colour = positivity,
      data=ms) + 
  geom_line(aes(group=positivity)) +
  facet_grid(.~utterance) +
  xlab("state (0=worst)") +
  ylab("proportion of\n acceptances") +
  geom_errorbar(aes(ymin=ci_lower,ymax=ci_upper,width=.1)) +
  theme_few(base_size = 16) +
  scale_color_ptol(name="")+
  # scale_color_solarized() +
  theme(legend.position = "bottom") +
  geom_hline(yintercept = .5, lty=2)

ggsave(litsem_plot, file = "literal_semantics.png", width = 7, height = 3,
       path = here::here(file_path))
```

```{r litsemPlotPlacement, echo=FALSE, fig.width = 10, fig.height = 4, out.width = "\\textwidth", fig.pos = "!h", fig.cap = "Semantic measurement results. Proportion of acceptances of utterance types (shown in different colors) combined with target words (shown in different facets) given the true state represented on a scale of hearts. Error bars represent 95\\% confidence intervals.", fig.scap="Semantic measurement results for Chapter 4."}
png::readPNG(here::here(file_path, "literal_semantics.png")) %>%
  grid::grid.raster()
```

## Data analysis

We used `r cite_r(here::here(file_path, "politeness.bib"))` for all our analyses.

## Full statistics on human data

```{r brmTab, results='asis'}
apa_table(brm.tab, caption= "Predictor mean estimates with standard deviation and 95% credible interval information for a Bayesian linear mixed-effects model predicting negation production based on true state and speaker goal (with both-goal as the reference level).")

# brm.tab %>%
#   kable("latex", booktabs = T, escape=F,
#         caption = "Predictor mean estimates with standard deviation and 95\% credible interval information for a Bayesian linear mixed-effects model predicting negation production based on true state and speaker goal (with both-goal as the reference level).",
#         caption.short = "Predictor mean estimates with standard deviation and 95\% credible interval information for a Bayesian linear mixed-effects model predicting negation production ") %>%
#   row_spec(row = 0, bold = TRUE)
  # column_spec(column = 1, width = "4cm")
```

We used Bayesian linear mixed-effects models [`brms` package in R; @R-brms] using crossed random effects of true state and goal with maximal random effects structure [@gelman2006data; @barr2013random]. The full statistics are shown in Table \@ref(tab:brmTab).


## Model fitting and inferred parameters

```{r otherParams, results='asis'}
other_tab <- d_other_s %>%
  mutate(model = fct_relevel(model, "inf", "soc", "pres", "inf_pres", "inf_soc", "soc_pres", "full")) %>%
    mutate(model = case_when(
    model == "inf" ~ "informational only", 
    model == "soc" ~ "social only", 
    model == "pres" ~ "presentational only", 
    model == "inf_pres" ~ "informational, presentational", 
    model == "inf_soc" ~ "informational, social", 
    model == "soc_pres" ~ "social, presentational", 
    model == "full" ~ "informational, social, presentational" 
  )) %>%
  mutate(param = round(param, digits=2)) %>%
  spread(parameter, param)

colnames(other_tab) <- c("Model", "Cost of negation", "Speaker optimality")

# apa_table(other_tab, escape=FALSE, caption = "Inferred negation cost and speaker optimality parameters for all model variants.")

other_tab %>% 
  kable("latex", booktabs = T, escape=F,
        caption = "Inferred negation cost and speaker optimality parameters for all model variants.",
        caption.short = "Other inferred parameters for all model variants.") %>% 
  row_spec(row = 0, bold = TRUE)
```

Other than speaker goal mixture weights explained in the main text (shown in Table \@ref(tab:phi)), the full model has two global parameters: the speaker's soft-max parameter $\lambda_{S_2}$ and soft-max paramater of the hypothetical speaker that the pragmatic listener reasons about $\lambda_{S_1}$.
$\lambda_{S_1}$ was 1, and $\lambda_{S_2}$ was inferred from the data: 
We put a prior that was consistent with those used for similar models in this model class: $\lambda_{S_2}$ ~ $Uniform(0,20)$.
Finally, we incorporate the literal semantics data into the RSA model by maintaining uncertainty about the semantic weight of utterance $w$ for state $s$, for each of the states and utterances, and assuming a Beta-Binomial linking function between these weights and the literal semantics data (see *Literal semantics task* above).
We infer the posterior distribution over all of the model parameters and generate model predictions based on this posterior distribution using Bayesian data analysis [@lee2014]. 
We ran 4 MCMC chains for 80,000 iterations, discarding the first 40,000 for burnin. 
The inferred values of parameters are shown in Table \@ref(tab:otherParams).

## Data Availability

Our model, preregistration of hypotheses, procedure, data, and analyses are available at \url{https://github.com/ejyoon/polite_speaker}. 

## Supplemental Figures

```{r utterance, echo=FALSE}
plot.utt <- ggplot(data=ms_utt %>%
                     filter(source == "data" | model == "full") %>%
  mutate(positivity = fct_recode(positivity, 
                               "It was ~" = "yes",
                               "It wasn't ~" = "not",
                               "It was ~" = "It was~",
                               "It wasn't ~" = "It wasn't~"
                               ),
         positivity = fct_relevel(positivity, "It was ~")) %>%
                     mutate(
                       # positivity = fct_relevel(positivity, "not"),
                            true_state = fct_recode(true_state,
                                                    "0 heart" = "0", 
                                                    "1 heart" = "1", 
                                                    "2 hearts" = "2", 
                                                    "3 hearts" = "3" 
                                                    ),
                            goal = fct_recode(goal, "kind" = "social")), 
       aes(x=utterance, y=prob, group = interaction(positivity, source), colour = positivity, linetype = source)) +
  geom_line()+
  facet_grid(goal~true_state, labeller = labeller(goal = label_both)) +
  xlab("utterance") +
  ylab("proportion chosen") +
  geom_linerange(aes(ymin=ci_lower,ymax=ci_upper), position="dodge") +
  geom_hline(yintercept=.1, lty=2) +
  ylim(0,1)+
  scale_color_ptol()+
  # scale_color_solarized(labels = c("It wasn't~","It was~"))+
  ggthemes::theme_few()+
  theme(axis.text.x = element_text(angle = 45, vjust=0.5),
        legend.position = "bottom") +
  guides(colour=guide_legend(title="utterance type")) +
  scale_linetype_discrete(labels = c("data", "model"))

ggsave("speaker_production_utt_wMod.png", plot = plot.utt, width = 7, height = 5,
       path = here::here(file_path))
```

```{r utterancePlacement, echo=FALSE, out.width = "\\textwidth", fig.pos = "!h", fig.height=6, fig.cap="Experimental results (solid lines) and fitted predictions from the full model (dashed lines) for speaker production. Proportion of utterances chosen (utterance type – direct vs. indirect – in different colors and words shown on x-axis) given the true states (columns) and speaker goals (rows). Error bars represent 95\\% confidence intervals for the data and 95\\% highest density intervals for the model. Black dotted line represents the chance level.", fig.scap="Full comparison between model predictions and experimental results from Chapter 4."}
png::readPNG(here::here(file_path, "speaker_production_utt_wMod.png")) %>%
  grid::grid.raster()
```

```{r comparisonAllPlot, echo=FALSE}
plot.comp.all <- ms_utt %>%  
  mutate(model = case_when(
    model == "NA" ~ "Human data", 
    model == "inf" ~ "model: \ninformational \nonly", 
    model == "soc" ~ "model: \nsocial only", 
    model == "pres" ~ "model: \npresentational \nonly", 
    model == "inf_pres" ~ "model: \ninformational, \npresentational", 
    model == "inf_soc" ~ "model: \ninformational, \nsocial", 
    model == "soc_pres" ~ "model: \nsocial, \npresentational", 
    model == "full" ~ "model: \ninformational, \nsocial, \npresentational" 
  ),
  model = fct_relevel(model, "model: \ninformational \nonly", "model: \nsocial only", "model: \npresentational \nonly", "model: \nsocial, \npresentational", "model: \ninformational, \nsocial", "model: \ninformational, \npresentational", "model: \ninformational, \nsocial, \npresentational")) %>%
  mutate(positivity = fct_recode(positivity, 
                               "It was ~" = "yes",
                               "It wasn't ~" = "not",
                               "It was ~" = "It was~",
                               "It wasn't ~" = "It wasn't~"
                               ),
         positivity = fct_relevel(positivity, "It was ~")) %>%
  mutate(
    # positivity = fct_recode(positivity,
    #                              "It was ~" = "yes",
    #                              "It wasn't ~" = "not"),
         # positivity = fct_relevel(positivity, "It wasn't ~"),
         goal = fct_recode(goal, "kind" = "social") 
         ) %>%
  filter(true_state == "0") %>%
  ggplot(., 
       aes(x=utterance, y=prob, fill=positivity, 
           # group = interaction(positive, source), linetype = forcats::fct_rev(positive),
           group = positivity,
           colour = positivity)) +
  geom_hline(yintercept=.125, lty=2, color="gray") +
  geom_line()+
  facet_grid(goal~model, labeller = labeller(goal = label_both)) +
  xlab("utterance") +
  geom_linerange(aes(ymin=ci_lower,ymax=ci_upper), position="dodge") +
  ylim(0,.7)+
  scale_color_ptol(guide=FALSE)+
  # scale_color_solarized(guide=FALSE)+
  ggthemes::theme_few(base_size = 15)+
  ylab("proportion chosen") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
        legend.position="bottom") +
  guides(color=guide_legend(title=NULL))

ggsave("model_comparisons_all.png", plot = plot.comp.all, width = 11, height = 6.5,
       path = here::here(file_path))

```

```{r comparisonAllPlacement, echo=FALSE, out.width = "\\textwidth", fig.pos = "!h", fig.width=11, fig.height=7, fig.cap="Comparison of predictions for proportion of utterances chosen by pragmatic speaker from possible model variants (left) and human data (rightmost) for average proportion of negation produced among all utterances, given true state of 0 heart and speaker with a goal to be informative (top), kind (middle), or both (bottom). Gray dotted line indicates chance level at 12.5\\%.", fig.scap="Full comparison of model variants for all conditions from the experiment in Chapter 4."}
png::readPNG(here::here(file_path, "model_comparisons_all.png")) %>%
  grid::grid.raster()
```


```{r negation, echo=FALSE}
plot.neg <- ms_neg %>%  
  mutate(model = case_when(
    model == "NA" ~ "Human data", 
    model == "inf" ~ "model: \ninformational \nonly", 
    model == "soc" ~ "model: \nsocial only", 
    model == "pres" ~ "model: \npresentational \nonly", 
    model == "inf_pres" ~ "model: \ninformational, \npresentational", 
    model == "inf_soc" ~ "model: \ninformational, \nsocial", 
    model == "soc_pres" ~ "model: \nsocial, \npresentational", 
    model == "full" ~ "model: \ninformational, \nsocial, \npresentational" 
  ),
  model = fct_relevel(model, "model: \ninformational \nonly", "model: \nsocial only", "model: \npresentational \nonly", "model: \nsocial, \npresentational", "model: \ninformational, \nsocial", "model: \ninformational, \npresentational", "model: \ninformational, \nsocial, \npresentational")) %>%
  ggplot(., 
       aes(x=true_state, y=prob, color = goal, group=goal)) +
  geom_line(stat="identity", position=position_dodge()) +
  xlab("true state") +
  ylab("proportion negation") +
  geom_linerange(aes(ymin=ci_lower,ymax=ci_upper), position=position_dodge(width=.05)) +
  theme_few(base_size = 15)+
  scale_color_solarized() +
  facet_grid(.~model) +
  theme(legend.position="bottom")

ggsave("speaker_production_neg_wMod.png", plot = plot.neg, width = 10, height = 5,
       path = here::here(file_path))
```

```{r negationPlacement, echo=FALSE, fig.width=11, fig.height=4, out.width = "\\textwidth", fig.pos = "!h", fig.cap="Experimental results (left) and fitted model predictions (right) for average proportion of negation produced among all utterances, given true states (x-axis) and goals (colors).", fig.scap="Comparison of expected proportion of negation from model predictions and experimental results from Chapter 4."}
png::readPNG(here::here(file_path, "speaker_production_neg_wMod.png")) %>%
  grid::grid.raster()
```
